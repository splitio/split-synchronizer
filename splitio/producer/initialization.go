package producer

import (
	"fmt"
	"os"
	"strings"
	"sync"
	"time"

	cfg "github.com/splitio/go-split-commons/v4/conf"
	"github.com/splitio/go-split-commons/v4/dtos"
	"github.com/splitio/go-split-commons/v4/provisional"
	"github.com/splitio/go-split-commons/v4/service/api"
	"github.com/splitio/go-split-commons/v4/telemetry"
	"github.com/splitio/go-toolkit/v5/logging"

	"github.com/splitio/go-split-commons/v4/storage/inmemory"
	"github.com/splitio/go-split-commons/v4/storage/redis"
	"github.com/splitio/go-split-commons/v4/synchronizer"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/impressionscount"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/segment"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/split"
	"github.com/splitio/go-split-commons/v4/tasks"
	"github.com/splitio/split-synchronizer/v4/conf"
	"github.com/splitio/split-synchronizer/v4/log"
	"github.com/splitio/split-synchronizer/v4/splitio"
	"github.com/splitio/split-synchronizer/v4/splitio/admin"
	"github.com/splitio/split-synchronizer/v4/splitio/common"
	ssync "github.com/splitio/split-synchronizer/v4/splitio/common/sync"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/evcalc"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/storage"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/task"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/worker"
	"github.com/splitio/split-synchronizer/v4/splitio/util"
)

func gracefulShutdownProducer(sigs chan os.Signal, gracefulShutdownWaitingGroup *sync.WaitGroup, syncManager synchronizer.Manager) {
	<-sigs

	log.PostShutdownMessageToSlack(false)

	fmt.Println("\n\n * Starting graceful shutdown")
	fmt.Println("")

	// Stopping Sync Manager in charge of PeriodicFetchers and PeriodicRecorders as well as Streaming
	fmt.Println(" -> Sending STOP to Synchronizer")
	syncManager.Stop()

	// Healthcheck - Emit task stop signal
	fmt.Println(" -> Sending STOP to healthcheck goroutine")
	// task.StopHealtcheck()

	fmt.Println(" * Waiting goroutines stop")
	gracefulShutdownWaitingGroup.Wait()

	fmt.Println(" * Shutting it down - see you soon!")
	os.Exit(splitio.SuccessfulOperation)
}

// Start initialize the producer mode
func Start(logger logging.LoggerInterface, sigs chan os.Signal, gracefulShutdownWaitingGroup *sync.WaitGroup) {
	// Getting initial config data
	advanced := conf.ParseAdvancedOptions()
	advanced.EventsBulkSize = conf.Data.EventsPerPost
	advanced.HTTPTimeout = int(conf.Data.HTTPTimeout)
	advanced.ImpressionsBulkSize = conf.Data.ImpressionsPerPost
	advanced.StreamingEnabled = conf.Data.StreamingEnabled
	metadata := util.GetMetadata()

	clientKey, err := util.GetClientKey(conf.Data.APIKey)
	if err != nil {
		logger.Error(err)
		os.Exit(1) // TODO(mredolatti): set an appropriate exitcode here
	}

	// Setup fetchers & recorders
	splitAPI := api.NewSplitAPI(conf.Data.APIKey, advanced, logger, metadata)

	// Check if apikey is valid
	if !isValidApikey(splitAPI.SplitFetcher) {
		logger.Error("Invalid apikey! Aborting execution.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Redis Storages
	redisOptions, err := parseRedisOptions()
	if err != nil {
		logger.Error("Failed to instantiate redis client.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}
	redisClient, err := redis.NewRedisClient(redisOptions, logger)
	if err != nil {
		logger.Error("Failed to instantiate redis client.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Instantiating storages
	miscStorage := redis.NewMiscStorage(redisClient, logger)
	err = sanitizeRedis(miscStorage, logger)
	if err != nil {
		logger.Error("Failed when trying to clean up redis. Aborting execution.")
		logger.Error(err.Error())
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Handle dual telemetry:
	// - telemetry generated by split-sync
	// - telemetry generated by sdks and picked up by split-sync
	syncTelemetryStorage, _ := inmemory.NewTelemetryStorage()
	sdkTelemetryStorage := storage.NewRedisTelemetryCosumerclient(redisClient, logger)

	// These storages are forwarded to the dashboard, the sdk-telemetry is irrelevant there
	storages := common.Storages{
		SplitStorage:          redis.NewSplitStorage(redisClient, logger),
		SegmentStorage:        redis.NewSegmentStorage(redisClient, logger),
		LocalTelemetryStorage: syncTelemetryStorage,
		ImpressionStorage:     redis.NewImpressionStorage(redisClient, dtos.Metadata{}, logger),
		EventStorage:          redis.NewEventsStorage(redisClient, dtos.Metadata{}, logger),
	}

	// Creating Workers and Tasks
	eventRecorder := worker.NewEventRecorderMultiple(storages.EventStorage, splitAPI.EventRecorder, syncTelemetryStorage, logger)
	workers := synchronizer.Workers{
		SplitFetcher: split.NewSplitFetcher(storages.SplitStorage, splitAPI.SplitFetcher, logger, syncTelemetryStorage),
		SegmentFetcher: segment.NewSegmentFetcher(storages.SplitStorage, storages.SegmentStorage, splitAPI.SegmentFetcher,
			logger, syncTelemetryStorage),
		EventRecorder: eventRecorder,
		TelemetryRecorder: telemetry.NewTelemetrySynchronizer(syncTelemetryStorage, splitAPI.TelemetryRecorder,
			storages.SplitStorage, storages.SegmentStorage, logger, metadata, syncTelemetryStorage),
	}
	splitTasks := synchronizer.SplitTasks{
		SplitSyncTask: tasks.NewFetchSplitsTask(workers.SplitFetcher, conf.Data.SplitsFetchRate, logger),
		SegmentSyncTask: tasks.NewFetchSegmentsTask(workers.SegmentFetcher, conf.Data.SegmentFetchRate,
			advanced.SegmentWorkers, advanced.SegmentQueueSize, logger),
		TelemetrySyncTask: tasks.NewRecordTelemetryTask(workers.TelemetryRecorder, conf.Data.MetricsPostRate, logger),
		EventSyncTask: tasks.NewRecordEventsTasks(workers.EventRecorder, advanced.EventsBulkSize, conf.Data.EventsPostRate,
			logger, conf.Data.EventsThreads),
	}

	impressionListenerEnabled := strings.TrimSpace(conf.Data.ImpressionListener.Endpoint) != ""
	managerConfig := cfg.ManagerConfig{
		ImpressionsMode: conf.Data.ImpressionsMode,
		OperationMode:   cfg.ProducerSync,
		ListenerEnabled: impressionListenerEnabled,
	}

	var impressionsCounter *provisional.ImpressionsCounter
	if conf.Data.ImpressionsMode == cfg.ImpressionsModeOptimized {
		impressionsCounter = provisional.NewImpressionsCounter()
		workers.ImpressionsCountRecorder = impressionscount.NewRecorderSingle(impressionsCounter, splitAPI.ImpressionRecorder, metadata,
			logger, syncTelemetryStorage)
		splitTasks.ImpressionsCountSyncTask = tasks.NewRecordImpressionsCountTask(workers.ImpressionsCountRecorder, logger)
	}
	impressionRecorder, err := worker.NewImpressionRecordMultiple(storages.ImpressionStorage, splitAPI.ImpressionRecorder, syncTelemetryStorage,
		logger, managerConfig, impressionsCounter)
	if err != nil {
		logger.Error(err)
		os.Exit(splitio.ExitTaskInitialization)
	}
	splitTasks.ImpressionSyncTask = tasks.NewRecordImpressionsTasks(impressionRecorder, conf.Data.ImpressionsPostRate, logger,
		advanced.ImpressionsBulkSize, conf.Data.ImpressionsThreads)

	sdkTelemetryWorker := worker.NewTelemetryMultiWorker(logger, sdkTelemetryStorage, splitAPI.TelemetryRecorder)
	sdkTelemetryTask := task.NewTelemetrySyncTask(sdkTelemetryWorker, logger, 10)
	syncImpl := ssync.NewSynchronizer(advanced, splitTasks, workers, logger, nil, []tasks.Task{sdkTelemetryTask})
	managerStatus := make(chan int, 1)
	syncManager, err := synchronizer.NewSynchronizerManager(
		syncImpl,
		logger,
		advanced,
		splitAPI.AuthClient,
		storages.SplitStorage,
		managerStatus,
		syncTelemetryStorage,
		metadata,
		&clientKey,
	)

	if err != nil {
		logger.Error(err)
		os.Exit(splitio.ExitTaskInitialization)
	}

	// Producer mode - graceful shutdown
	go gracefulShutdownProducer(sigs, gracefulShutdownWaitingGroup, syncManager)

	rtm := common.NewRuntime()
	impressionEvictionMonitor := evcalc.New(1) // TODO(mredolatti): set the correct thread count
	eventEvictionMonitor := evcalc.New(1)      // TODO(mredolatti): set the correct thread count

	// --------------------------- ADMIN DASHBOARD ------------------------------
	adminServer, err := admin.NewServer(&admin.Options{
		Host:              "0.0.0.0",
		Port:              conf.Data.Producer.Admin.Port,
		Name:              "Split Synchronizer dashboard (producer mode)",
		Proxy:             false,
		Username:          conf.Data.Proxy.AdminUsername,
		Password:          conf.Data.Producer.Admin.Password,
		Logger:            logger,
		Storages:          storages,
		ImpressionsEvCalc: impressionEvictionMonitor,
		EventsEvCalc:      eventEvictionMonitor,
		Runtime:           rtm,
	})
	if err != nil {
		panic(err.Error())
	}
	go adminServer.ListenAndServe()

	// Run Sync Manager
	before := time.Now()
	go syncManager.Start()
	select {
	case status := <-managerStatus:
		switch status {
		case synchronizer.Ready:
			logger.Info("Synchronizer tasks started")
			workers.TelemetryRecorder.SynchronizeConfig(
				telemetry.InitConfig{
					AdvancedConfig: advanced,
					TaskPeriods: cfg.TaskPeriods{
						SplitSync:      conf.Data.SplitsFetchRate,
						SegmentSync:    conf.Data.SegmentFetchRate,
						ImpressionSync: conf.Data.ImpressionsPostRate,
						TelemetrySync:  10, // TODO(mredolatti): Expose this as a config option
					},
					ManagerConfig: cfg.ManagerConfig{
						ImpressionsMode: conf.Data.ImpressionsMode,
						ListenerEnabled: impressionListenerEnabled,
					},
				},
				time.Now().Sub(before).Milliseconds(),
				map[string]int64{conf.Data.APIKey: 1},
				nil,
			)
		case synchronizer.Error:
			logger.Error("Error starting synchronizer")
			os.Exit(splitio.ExitTaskInitialization)
		}
	}
	// task.InitializeEvictionCalculator()

	if impressionListenerEnabled {
		for i := 0; i < conf.Data.ImpressionsThreads; i++ {
			// go task.PostImpressionsToListener(recorder.ImpressionListenerSubmitter{Endpoint: conf.Data.ImpressionListener.Endpoint})
		}
	}

	// go task.CheckEnvirontmentStatus(gracefulShutdownWaitingGroup, storages.SplitStorage, httpClients)

	// Keeping service alive
	startLoop(500)
}
