package producer

import (
	"fmt"
	"os"
	"strings"
	"sync"
	"time"

	cfg "github.com/splitio/go-split-commons/v4/conf"
	"github.com/splitio/go-split-commons/v4/dtos"
	"github.com/splitio/go-split-commons/v4/provisional"
	"github.com/splitio/go-split-commons/v4/service/api"
	"github.com/splitio/go-split-commons/v4/telemetry"

	"github.com/splitio/go-split-commons/v4/storage/inmemory"
	"github.com/splitio/go-split-commons/v4/storage/redis"
	"github.com/splitio/go-split-commons/v4/synchronizer"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/impressionscount"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/segment"
	"github.com/splitio/go-split-commons/v4/synchronizer/worker/split"
	"github.com/splitio/go-split-commons/v4/tasks"
	"github.com/splitio/split-synchronizer/v4/conf"
	"github.com/splitio/split-synchronizer/v4/log"
	"github.com/splitio/split-synchronizer/v4/splitio"
	"github.com/splitio/split-synchronizer/v4/splitio/common"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/storage"
	wsync "github.com/splitio/split-synchronizer/v4/splitio/producer/sync"
	"github.com/splitio/split-synchronizer/v4/splitio/producer/worker"
	"github.com/splitio/split-synchronizer/v4/splitio/recorder"
	"github.com/splitio/split-synchronizer/v4/splitio/task"
	"github.com/splitio/split-synchronizer/v4/splitio/util"
	"github.com/splitio/split-synchronizer/v4/splitio/web/admin"
)

func gracefulShutdownProducer(sigs chan os.Signal, gracefulShutdownWaitingGroup *sync.WaitGroup, syncManager synchronizer.Manager) {
	<-sigs

	log.PostShutdownMessageToSlack(false)

	fmt.Println("\n\n * Starting graceful shutdown")
	fmt.Println("")

	// Stopping Sync Manager in charge of PeriodicFetchers and PeriodicRecorders as well as Streaming
	fmt.Println(" -> Sending STOP to Synchronizer")
	syncManager.Stop()

	// Healthcheck - Emit task stop signal
	fmt.Println(" -> Sending STOP to healthcheck goroutine")
	task.StopHealtcheck()

	fmt.Println(" * Waiting goroutines stop")
	gracefulShutdownWaitingGroup.Wait()

	fmt.Println(" * Shutting it down - see you soon!")
	os.Exit(splitio.SuccessfulOperation)
}

// Start initialize the producer mode
func Start(sigs chan os.Signal, gracefulShutdownWaitingGroup *sync.WaitGroup) {
	// Getting initial config data
	advanced := conf.ParseAdvancedOptions()
	advanced.EventsBulkSize = conf.Data.EventsPerPost
	advanced.HTTPTimeout = int(conf.Data.HTTPTimeout)
	advanced.ImpressionsBulkSize = conf.Data.ImpressionsPerPost
	advanced.StreamingEnabled = conf.Data.StreamingEnabled
	metadata := util.GetMetadata()

	clientKey, err := util.GetClientKey(conf.Data.APIKey)
	if err != nil {
		log.Instance.Error(err)
		os.Exit(1) // TODO(mredolatti): set an appropriate exitcode here
	}

	// Setup fetchers & recorders
	splitAPI := api.NewSplitAPI(conf.Data.APIKey, advanced, log.Instance, metadata)

	// Check if apikey is valid
	if !isValidApikey(splitAPI.SplitFetcher) {
		log.Instance.Error("Invalid apikey! Aborting execution.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Redis Storages
	redisOptions, err := parseRedisOptions()
	if err != nil {
		log.Instance.Error("Failed to instantiate redis client.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}
	redisClient, err := redis.NewRedisClient(redisOptions, log.Instance)
	if err != nil {
		log.Instance.Error("Failed to instantiate redis client.")
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Instantiating storages
	miscStorage := redis.NewMiscStorage(redisClient, log.Instance)
	err = sanitizeRedis(miscStorage, log.Instance)
	if err != nil {
		log.Instance.Error("Failed when trying to clean up redis. Aborting execution.")
		log.Instance.Error(err.Error())
		os.Exit(splitio.ExitRedisInitializationFailed)
	}

	// Handle dual telemetry:
	// - telemetry generated by split-sync
	// - telemetry generated by sdks and picked up by split-sync
	syncTelemetryStorage, _ := inmemory.NewTelemetryStorage()
	sdkTelemetryStorage := storage.NewRedisTelemetryCosumerclient(redisClient, log.Instance)

	// These storages are forwarded to the dashboard, the sdk-telemetry is irrelevant there
	storages := common.Storages{
		SplitStorage:          redis.NewSplitStorage(redisClient, log.Instance),
		SegmentStorage:        redis.NewSegmentStorage(redisClient, log.Instance),
		LocalTelemetryStorage: syncTelemetryStorage,
		ImpressionStorage:     redis.NewImpressionStorage(redisClient, dtos.Metadata{}, log.Instance),
		EventStorage:          redis.NewEventsStorage(redisClient, dtos.Metadata{}, log.Instance),
	}

	// Creating Workers and Tasks
	eventRecorder := worker.NewEventRecorderMultiple(storages.EventStorage, splitAPI.EventRecorder, syncTelemetryStorage, log.Instance)
	workers := synchronizer.Workers{
		SplitFetcher: split.NewSplitFetcher(storages.SplitStorage, splitAPI.SplitFetcher, log.Instance, syncTelemetryStorage),
		SegmentFetcher: segment.NewSegmentFetcher(storages.SplitStorage, storages.SegmentStorage, splitAPI.SegmentFetcher,
			log.Instance, syncTelemetryStorage),
		EventRecorder: eventRecorder,
		TelemetryRecorder: telemetry.NewTelemetrySynchronizer(syncTelemetryStorage, splitAPI.TelemetryRecorder,
			storages.SplitStorage, storages.SegmentStorage, log.Instance, metadata, syncTelemetryStorage),
	}
	splitTasks := synchronizer.SplitTasks{
		SplitSyncTask: tasks.NewFetchSplitsTask(workers.SplitFetcher, conf.Data.SplitsFetchRate, log.Instance),
		SegmentSyncTask: tasks.NewFetchSegmentsTask(workers.SegmentFetcher, conf.Data.SegmentFetchRate,
			advanced.SegmentWorkers, advanced.SegmentQueueSize, log.Instance),
		TelemetrySyncTask: tasks.NewRecordTelemetryTask(workers.TelemetryRecorder, conf.Data.MetricsPostRate, log.Instance),
		EventSyncTask: tasks.NewRecordEventsTasks(workers.EventRecorder, advanced.EventsBulkSize, conf.Data.EventsPostRate,
			log.Instance, conf.Data.EventsThreads),
	}

	impressionListenerEnabled := strings.TrimSpace(conf.Data.ImpressionListener.Endpoint) != ""
	managerConfig := cfg.ManagerConfig{
		ImpressionsMode: conf.Data.ImpressionsMode,
		OperationMode:   cfg.ProducerSync,
		ListenerEnabled: impressionListenerEnabled,
	}

	var impressionsCounter *provisional.ImpressionsCounter
	if conf.Data.ImpressionsMode == cfg.ImpressionsModeOptimized {
		impressionsCounter = provisional.NewImpressionsCounter()
		workers.ImpressionsCountRecorder = impressionscount.NewRecorderSingle(impressionsCounter, splitAPI.ImpressionRecorder, metadata,
			log.Instance, syncTelemetryStorage)
		splitTasks.ImpressionsCountSyncTask = tasks.NewRecordImpressionsCountTask(workers.ImpressionsCountRecorder, log.Instance)
	}
	impressionRecorder, err := worker.NewImpressionRecordMultiple(storages.ImpressionStorage, splitAPI.ImpressionRecorder, syncTelemetryStorage,
		log.Instance, managerConfig, impressionsCounter)
	if err != nil {
		log.Instance.Error(err)
		os.Exit(splitio.ExitTaskInitialization)
	}
	splitTasks.ImpressionSyncTask = tasks.NewRecordImpressionsTasks(impressionRecorder, conf.Data.ImpressionsPostRate, log.Instance,
		advanced.ImpressionsBulkSize, conf.Data.ImpressionsThreads)

	sdkTelemetryWorker := worker.NewTelemetryMultiWorker(log.Instance, sdkTelemetryStorage, splitAPI.TelemetryRecorder)
	syncImpl := wsync.NewSynchronizer(advanced, splitTasks, workers, log.Instance, nil, sdkTelemetryWorker, 10)
	managerStatus := make(chan int, 1)
	syncManager, err := synchronizer.NewSynchronizerManager(
		syncImpl,
		log.Instance,
		advanced,
		splitAPI.AuthClient,
		storages.SplitStorage,
		managerStatus,
		syncTelemetryStorage,
		metadata,
		&clientKey,
	)

	if err != nil {
		log.Instance.Error(err)
		os.Exit(splitio.ExitTaskInitialization)
	}

	// Producer mode - graceful shutdown
	go gracefulShutdownProducer(sigs, gracefulShutdownWaitingGroup, syncManager)

	// --------------------------- ADMIN DASHBOARD ------------------------------
	// WebAdmin configuration
	waOptions := &admin.WebAdminOptions{
		Port:          conf.Data.Producer.Admin.Port,
		AdminUsername: conf.Data.Producer.Admin.Username,
		AdminPassword: conf.Data.Producer.Admin.Password,
		DebugOn:       conf.Data.Logger.DebugOn,
	}

	// Run WebAdmin Server
	httpClients := common.HTTPClients{
		AuthClient:   api.NewHTTPClient(conf.Data.APIKey, advanced, advanced.AuthServiceURL, log.Instance, metadata),
		SdkClient:    api.NewHTTPClient(conf.Data.APIKey, advanced, advanced.SdkURL, log.Instance, metadata),
		EventsClient: api.NewHTTPClient(conf.Data.APIKey, advanced, advanced.EventsURL, log.Instance, metadata),
	}
	recorders := common.Recorders{
		Impression: impressionRecorder,
		Event:      eventRecorder,
	}
	admin.StartAdminWebAdmin(waOptions, storages, httpClients, recorders)
	// ---------------------------------------------------------------------------

	// Run Sync Manager
	before := time.Now()
	go syncManager.Start()
	select {
	case status := <-managerStatus:
		switch status {
		case synchronizer.Ready:
			log.Instance.Info("Synchronizer tasks started")
			workers.TelemetryRecorder.SynchronizeConfig(
				telemetry.InitConfig{
					AdvancedConfig: advanced,
					TaskPeriods: cfg.TaskPeriods{
						SplitSync:      conf.Data.SplitsFetchRate,
						SegmentSync:    conf.Data.SegmentFetchRate,
						ImpressionSync: conf.Data.ImpressionsPostRate,
						TelemetrySync:  10, // TODO(mredolatti): Expose this as a config option
					},
					ManagerConfig: cfg.ManagerConfig{
						ImpressionsMode: conf.Data.ImpressionsMode,
						ListenerEnabled: impressionListenerEnabled,
					},
				},
				time.Now().Sub(before).Milliseconds(),
				map[string]int64{conf.Data.APIKey: 1},
				nil,
			)
		case synchronizer.Error:
			log.Instance.Error("Error starting synchronizer")
			os.Exit(splitio.ExitTaskInitialization)
		}
	}
	task.InitializeEvictionCalculator()

	if impressionListenerEnabled {
		for i := 0; i < conf.Data.ImpressionsThreads; i++ {
			go task.PostImpressionsToListener(recorder.ImpressionListenerSubmitter{Endpoint: conf.Data.ImpressionListener.Endpoint})
		}
	}

	go task.CheckEnvirontmentStatus(gracefulShutdownWaitingGroup, storages.SplitStorage, httpClients)

	// Keeping service alive
	startLoop(500)
}
